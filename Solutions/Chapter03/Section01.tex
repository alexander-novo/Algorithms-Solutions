%!TEX root = ../../Algorithms.tex
\documentclass[Chapter03]{subfiles}

\begin{document}
	\subsection{Asymptotic Notation}

	\begin{enumerate}[leftmargin=\labelsep]
		% 3.1-1
		\item Let $f(n)$ and $g(n)$ be asymptotically nonnegative functions. Using the basic definition of $\Theta$-notation, prove that $\max\{f(n), g(n)\} = \Theta(f(n) + g(n))$.
		\begin{answer}
			Let $n_0 = 1$ and $n \geq n_0$. Without loss of generality, assume $f(n) \leq g(n)$. Then we have
			\begin{align*}
				\max\{f(n), g(n)\} &= f(n)\\
					&\leq f(n) + g(n),
			\end{align*}
			and
			\begin{align*}
				\max\{f(n), g(n)\} &= f(n)\\
					&= \frac{1}{2}\qty(f(n) + f(n))\\
					&\geq \frac{1}{2}\qty(f(n) + g(n)),
			\end{align*}
			so for $c_1 = \frac{1}{2}$ and $c_2 = 1$, $\max\{f(n), g(n)\} = \Theta(f(n) + g(n))$.
		\end{answer}

		% 3.1-2
		\item Show that for any real constants $a$ and $b$, where $b > 0$,
		\[
			(n + a)^b = \Theta(n^b).
		\]
		\begin{answer}
			Let $n_0 = 1$ and $n \geq n_0$. When $a \geq 0$, we have
			\begin{align*}
				(n + a)^b &\leq (n + an)^b\\
					&= (1 + a)^bn^b,
			\end{align*}
			and
			\begin{align*}
				(n + a)^b &\geq (n + 0)^b\\
					&= n^b,
			\end{align*}
			so for $c_1 = 1$ and $c_2 = (1 + a)^b$, $(n + a)^b = \Theta(n^b)$. When $a < 0$, then all of the above inequalities are exactly flipped, so $(n + a)^b = \Theta(n^b)$ still.
		\end{answer}

		% 3.1-3
		\item Explain why the statement, ``The running time of algorithm $A$ is at least $\bigO(n^2)$,'' is meaningless.
		\begin{answer}
			Since $\bigO$-notation gives an upper bound, it implies upper bounds of anything greater than $n^2$ by itself.
		\end{answer}

		% 3.1-4
		\item Is $2^{n + 1} = \bigO(2^n)$? Is $2^{2n} = \bigO(2^n)$?
		\begin{answer}
			
		\end{answer}

		% 3.1-5
		\item Prove Theorem 3.1.
		\begin{theorem}[Theorem 3.1]
			For any two functions $f(n)$ and $g(n)$, we have $f(n) = \Theta(g(n))$ if and only if $f(n) = \bigO(g(n))$ and $f(n) = \Omega(g(n))$.
		\end{theorem}
		\begin{answer}
			
		\end{answer}

		% 3.1-6
		\item Prove that the running time of an algorithm is $\Theta(g(n))$ if and only if its worst-case running time is $\bigO(g(n))$ and its best-case running time is $\Omega(g(n))$.
		\begin{answer}
			
		\end{answer}

		% 3.1-7
		\item Prove that $o(g(n)) \cap \omega(g(n))$ is the empty set.
		\begin{answer}
			
		\end{answer}

		% 3.1-8
		\item We can extend our notation to the case of two parameters $n$ and $m$ that can go to infinity independently at different rates. For a given function $g(n,m)$, we denote by $\bigO(g(n,m))$ the set of functions
		\begin{align*}
			O(g(n,m)) = \{ f(n,m) \mid\ &\text{there exist positive constants $c, n_0$, and $m_0$}\\
				&\text{such that $0 \leq f(n,m) \leq cg(n,m)$}\\
				&\text{for all $n \geq n_0$ or $m \geq m_0$}\}.
		\end{align*}
			
		\begin{answer}
			
		\end{answer}

	\end{enumerate}

\end{document}