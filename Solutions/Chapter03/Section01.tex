%!TEX root = ../../Algorithms.tex
\documentclass[Chapter03]{subfiles}

\begin{document}
	\subsection{Asymptotic Notation}

	\begin{enumerate}
		% 3.1-1
		\item Let $f(n)$ and $g(n)$ be asymptotically nonnegative functions. Using the basic definition of $\Theta$-notation, prove that $\max\{f(n), g(n)\} = \Theta(f(n) + g(n))$.
		\begin{answer}
			Let $n_0 = 1$ and $n \geq n_0$. Without loss of generality, assume $f(n) \geq g(n)$. Then we have
			\begin{align*}
				\max\{f(n), g(n)\} &= f(n)\\
					&\leq f(n) + g(n),
			\end{align*}
			and
			\begin{align*}
				\max\{f(n), g(n)\} &= f(n)\\
					&= \frac{1}{2}\qty(f(n) + f(n))\\
					&\geq \frac{1}{2}\qty(f(n) + g(n)),
			\end{align*}
			so for $c_1 = \frac{1}{2}$ and $c_2 = 1$, $\max\{f(n), g(n)\} = \Theta(f(n) + g(n))$.
		\end{answer}

		% 3.1-2
		\item Show that for any real constants $a$ and $b$, where $b > 0$,
		\[
			(n + a)^b = \Theta(n^b).
		\]
		\begin{answer}
			Let $n_0 = 1$ and $n \geq n_0$. When $a \geq 0$, we have
			\begin{align*}
				(n + a)^b &\leq (n + an)^b\\
					&= (1 + a)^bn^b,
			\end{align*}
			and
			\begin{align*}
				(n + a)^b &\geq (n + 0)^b\\
					&= n^b,
			\end{align*}
			so for $c_1 = 1$ and $c_2 = (1 + a)^b$, $(n + a)^b = \Theta(n^b)$. When $a < 0$, then all of the above inequalities are exactly flipped, so $(n + a)^b = \Theta(n^b)$ still.
		\end{answer}

		% 3.1-3
		\item Explain why the statement, ``The running time of algorithm $A$ is at least $\bigO(n^2)$,'' is meaningless.
		\begin{answer}
			Since $\bigO$-notation gives an upper bound, it implies upper bounds of anything greater than $n^2$ by itself.
		\end{answer}

		% 3.1-4
		\item Is $2^{n + 1} = \bigO(2^n)$? Is $2^{2n} = \bigO(2^n)$?
		\begin{answer}
			Yes, $2^{n + 1} = \bigO(2^n)$, since $2^{n+1} = 2\cdot 2^n$, so for $c = 2$ we have $2^{n + 1} = \bigO(2^n)$. However, $2^{2n} \neq \bigO(2^n)$. If given $c > 0$, then $2^{2n} > c2^n$ for all $n > \lg(c)$, so $2^{2n} = \omega(2^n)$.
		\end{answer}

		% 3.1-5
		\item Prove Theorem 3.1.
		\begin{theorem}[Theorem 3.1]
			For any two functions $f(n)$ and $g(n)$, we have $f(n) = \Theta(g(n))$ if and only if $f(n) = \bigO(g(n))$ and $f(n) = \Omega(g(n))$.
		\end{theorem}
		\begin{answer}
			\boxed{\implies} Let $f,g$ be functions such that $f(n) = \Theta(g(n))$. Then there exist constants $n_0,c_1,c_2 > 0$ such that $0 \leq c_1g(n) \leq f(n) \leq c_2g(n)$ whenever $n \geq n_0$. The second inequality tells us that $f(n) = \Omega(g(n))$, and the third tells us that $f(n) = \bigO(g(n))$.

			\boxed{\impliedby} Let $f,g$ be functions such that $f(n) = \bigO(g(n))$ and $f(n) = \Omega(g(n))$. Then there exist constants $n_1,n_2,c_1,c_2$ such that $0 \leq f(n) \leq c_1g(n)$ whenever $n > n_1$ and $0 \leq c_2g(n) \leq f(n)$ whenever $n > n_2$. Then $0 \leq c_2g(n) \leq f(n) \leq c_1g(n)$ whenever $n \geq \max\{c_1,c_2\}$. Therefore, $f(n) = \Theta(g(n))$.
		\end{answer}

		% 3.1-6
		\item Prove that the running time of an algorithm is $\Theta(g(n))$ if and only if its worst-case running time is $\bigO(g(n))$ and its best-case running time is $\Omega(g(n))$.
		\begin{answer}
			\boxed{\implies} Assume that the running time of an algorithm is $\Theta(g(n))$. Then the worst-case running time and best-case running times are also $\Theta(g(n))$. The rest follows from Theorem 3.1.

			\boxed{\impliedby} Let $f(n)$ represent the running time of an algorithm, $f_{worst}(n)$ represent the running time of the algorithm in worst-case, and $f_{best}(n)$ represent the running time of the algorithm in the best-case. It should be intuitive that $f_{best}(n) \leq f(n) \leq f_{worst}(n)$. Assume that $f_{worst}(n) = \bigO(g(n))$ and $f_{best}(n) = \Omega(g(n))$. Then there exist constants $n_1,n_2,c_1,c_2 > 0$ such that $0 \leq f_{worst} \leq c_1g(n)$ whenever $n \geq n_1$ and $0 \leq c_2g(n) \leq f_{best}(n)$ whenever $n \geq n_2$. Then we have
			\[
				0 \leq c_2g(n) \leq f_{best}(n) \leq f(n) \leq f_{worst}(n) \leq c_1g(n)
			\]
			whenever $n \geq \max\{n_1,n_2\}$. Therefore the running time of the algorithm is $\Theta(g(n))$.
		\end{answer}

		% 3.1-7
		\item Prove that $o(g(n)) \cap \omega(g(n))$ is the empty set.
		\begin{answer}
			By contradiction, assume $o(g(n)) \cap \omega(g(n))$ is nonempty. Then there exists a function $f$ and constants $n_1,n_2 > 0$ such that $0 \leq f(n) < cg(n)$ whenever $n \geq n_1$ and $0 \leq cg(n) < f(n)$ whenever $n \geq n_2$ for all $c > 0$. But then $g(n) < f(n) < g(n)$ for all $n \geq \max\{n_1,n_2\}$ and $0 < 0$ (by subtracting $g(n)$ from both sides), which is a contradiction. Therefore $o(g(n)) \cap \omega(g(n))$ is the empty set.
		\end{answer}

		% 3.1-8
		\item We can extend our notation to the case of two parameters $n$ and $m$ that can go to infinity independently at different rates. For a given function $g(n,m)$, we denote by $\bigO(g(n,m))$ the set of functions
		\begin{align*}
			\bigO(g(n,m)) = \{ f(n,m) \mid\ &\text{there exist positive constants $c, n_0$, and $m_0$}\\
				&\text{such that $0 \leq f(n,m) \leq cg(n,m)$}\\
				&\text{for all $n \geq n_0$ or $m \geq m_0$}\}.
		\end{align*}
		Give corresponding definitions for $\Omega(g(n,m))$ and $\Theta(g(n,m))$.
		\begin{answer}
			We define $\Omega(g(n,m))$ and $\Theta(g(n,m))$ as
			\begin{align*}
				\Omega(g(n,m)) = \{ f(n,m) \mid\ &\text{there exist positive constants $c, n_0$, and $m_0$}\\
					&\text{such that $0 \leq cg(n,m) \leq f(n,m)$}\\
					&\text{for all $n \geq n_0$ or $m \geq m_0$}\},\\
				\Theta(g(n,m)) = \{ f(n,m) \mid\ &\text{there exist positive constants $c_1, c_2, n_0$, and $m_0$}\\
					&\text{such that $0 \leq c_1g(n,m) \leq f(n,m) \leq c_2g(n,m)$}\\
					&\text{for all $n \geq n_0$ or $m \geq m_0$}\}.
			\end{align*}
		\end{answer}

	\end{enumerate}

\end{document}